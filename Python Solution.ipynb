{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string,re\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from Class_replace_impute_encode import ReplaceImputeEncode\n",
    "from Class_tree import DecisionTree\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README\n",
    "\n",
    "These\tdata\tconsist of\t5,330\tconsumer\tcomplaints\t\n",
    "submitted\tto\tthe\tNTHSA\tfor\tsome\tHonda\tmakes\tin\tyears\t2001-2003.\n",
    "\n",
    "Problem\twas\tto\tbuild\tand\tvalidate\tthe\tbest\tmodel\tfor\tpredicting\tthe\t\n",
    "probability\tof\ta\tcrash\tbased\tupon\tthe\ttopic\tand\tsentiment\tmodel\tand\t\n",
    "upon\tthe\tother\tdata\tavailable\tin\tthe\tproject\tfile.\n",
    "\n",
    "This\tinvolved\tthe\tfollowing:\n",
    "1. Built\ta\tTopic\tModel\tthat\torganizes\tthese\tcomplaints\tinto\t7\tgroups.\n",
    "2. Scored\tthe\tSentiment\tfor\teach\tcomplaint.\n",
    "3. Merged\tthe\ttopic\tgroup\tinformation\tand\tsentiments\tback\tinto\tthe\toriginal\tdata\tfile.\n",
    "4. Built\tthe\tbest\tdecision\ttree\tto\tpredict\tthe\tprobability\tof\ta\tcrash.\n",
    "5. Downloaded\tthe\tlatest\tnews\ton\tthe\tJapanese\tairbag\tmanufacturer\t\n",
    "“Takata”\tfrom\tAPI and \tcommented\ton\thow\tthese\tarticles\tdo\tor\tdo\tnot\trelate\tto\tthe\tTopic\tgroups found in the earlier part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:/Users/Aparajit Koshal/Desktop/Finals/HondaComplaints.xlsx\")\n",
    "sw = pd.read_excel(\"C:/Users/Aparajit Koshal/Desktop/Finals/afinn_sentiment_words.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['description']= df['description'].str.replace('AIR BAG', 'AIRBAG')\n",
    "df= df.drop_duplicates('description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>NhtsaID</th>\n",
       "      <th>Make</th>\n",
       "      <th>Model</th>\n",
       "      <th>Year</th>\n",
       "      <th>State</th>\n",
       "      <th>abs</th>\n",
       "      <th>cruise</th>\n",
       "      <th>crash</th>\n",
       "      <th>mph</th>\n",
       "      <th>mileage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CONSUMER STATES FIRESTONE TIRE (NO SIZE) EXPER...</td>\n",
       "      <td>560001</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>CIVIC</td>\n",
       "      <td>2001</td>\n",
       "      <td>CA</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>31</td>\n",
       "      <td>85064.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE VEHICLE EXPERIENCES EXCESSIVE VIBRATION OF...</td>\n",
       "      <td>561194</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>ACCORD</td>\n",
       "      <td>2001</td>\n",
       "      <td>CA</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>70</td>\n",
       "      <td>87186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CONSUMER IS NOT HAPPY WITH THE DEALER DURING R...</td>\n",
       "      <td>562006</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>CIVIC</td>\n",
       "      <td>2001</td>\n",
       "      <td>WV</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>31</td>\n",
       "      <td>81110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CONSUMER IS NOT HAPPY WITH THE DEALER DURING R...</td>\n",
       "      <td>562066</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>CIVIC</td>\n",
       "      <td>2001</td>\n",
       "      <td>WV</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>31</td>\n",
       "      <td>81110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CONSUMER STATES WHEN HE PROCEEDED TO BACK UP, ...</td>\n",
       "      <td>562091</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>ACCORD</td>\n",
       "      <td>2001</td>\n",
       "      <td>PA</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>24</td>\n",
       "      <td>96792.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  NhtsaID   Make   Model  \\\n",
       "0  CONSUMER STATES FIRESTONE TIRE (NO SIZE) EXPER...   560001  HONDA   CIVIC   \n",
       "1  THE VEHICLE EXPERIENCES EXCESSIVE VIBRATION OF...   561194  HONDA  ACCORD   \n",
       "2  CONSUMER IS NOT HAPPY WITH THE DEALER DURING R...   562006  HONDA   CIVIC   \n",
       "3  CONSUMER IS NOT HAPPY WITH THE DEALER DURING R...   562066  HONDA   CIVIC   \n",
       "4  CONSUMER STATES WHEN HE PROCEEDED TO BACK UP, ...   562091  HONDA  ACCORD   \n",
       "\n",
       "   Year State abs cruise crash  mph  mileage  \n",
       "0  2001    CA   N      N     N   31  85064.0  \n",
       "1  2001    CA   N      N     N   70  87186.0  \n",
       "2  2001    WV   N      N     N   31  81110.0  \n",
       "3  2001    WV   N      N     N   31  81110.0  \n",
       "4  2001    PA   N      N     N   24  96792.0  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_dic = {}\n",
    "for i in range(len(sw)):\n",
    "    sentiment_dic[sw.iloc[i][0]] = sw.iloc[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_analyzer(s):\n",
    "    # Synonym List\n",
    "    syns = {'irrresponsible' :'irresponsible',\n",
    "              'wont':'would not', 'cant':'can not', 'cannot':'can not', \\\n",
    "              'couldnt':'could not', 'shouldnt':'should not', \\\n",
    "              'wouldnt':'would not', 'anticipate':'anticipate', 'airbag':'airbags'}\n",
    "    \n",
    "    # Preprocess String s  \n",
    "    s= s.lower()\n",
    "    s = s.replace('-', ' ')\n",
    "    s = s.replace(',', '. ')\n",
    "    s = s.replace('_', ' ')\n",
    "    s = s.replace(\"'nt\", \" not\")\n",
    "    s = s.replace(\"n't\", \" not\")\n",
    "    # Tokenize \n",
    "    tokens = word_tokenize(s)\n",
    "    tokens = [word.replace(',','') for word in tokens ]\n",
    "    tokens = [word for word in tokens if ('*' not in word) and \\\n",
    "              (\"''\" != word) and (\"``\" != word) and \\\n",
    "              (word!='description') and (word !='dtype') \\\n",
    "              and (word != 'object') and (word!=\"'s\")]\n",
    "    \n",
    "    # Map synonyms\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in syns:\n",
    "            tokens[i] = syns[tokens[i]]\n",
    "            \n",
    "    # Remove stop words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    punctuation = list(string.punctuation)+['..', '...']\n",
    "    pronouns = ['i', 'he', 'she', 'it', 'him', 'they', 'we', 'us', 'them']\n",
    "    custom_stop= [[\"quot\", \"amp\", \"could\", \"also\", \"even\", \"really\", \"one\", \\\n",
    "                    \"would\", \"get\", \"getting\", \"go\", \"going\", \"..\", \"...\", \\\n",
    "                    \"us\", \"area\", \"vegas\",\"oct\", \"place\", \"want\", \"get\", \\\n",
    "                    \"take\", \"end\", \"la\", \"gal\", \"get\", \"next\", \"though\", \\\n",
    "                    \"non\", \"seem\", \"use\", \"sep\", \"w/\", \"jul\", \"get\", \"go\",\"almost\", \"say\", \"tell\",\\\n",
    "                    \"own\", \"car\", \"xxx\", \"son\"  ]]\n",
    "    stop = stopwords.words('english') + punctuation + pronouns+ custom_stop\n",
    "    filtered_terms = [word for word in tokens if (word not in stop) and \\\n",
    "                  (len(word)>2) and (not word.replace('.','',1).isdigit()) \\\n",
    "                  and (not word.replace(\"'\",'',2).isdigit()) and re.sub('[^a-z\\s]',' ', str(word))]\n",
    "    # Lemmatization & Stemming - Stemming with WordNet POS\n",
    "    # Since lemmatization requires POS need to set POS\n",
    "    tagged_words = pos_tag(filtered_terms, lang='eng')\n",
    "    # Stemming with for terms without WordNet POS\n",
    "    \n",
    "    wn_tags = {'N':wn.NOUN, 'J':wn.ADJ, 'V':wn.VERB, 'R':wn.ADV}\n",
    "    wnl = WordNetLemmatizer()\n",
    "    stemmed_tokens = []\n",
    "    for tagged_token in tagged_words:\n",
    "        term = tagged_token[0]\n",
    "        pos  = tagged_token[1]\n",
    "        pos  = pos[0]\n",
    "        try:\n",
    "            pos   = wn_tags[pos]\n",
    "            stemmed_tokens.append(wnl.lemmatize(term, pos=pos))\n",
    "        except:\n",
    "            stemmed_tokens.append(stemmer.stem(term))\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_preprocessor(s):\n",
    "    # Preprocess String s\n",
    "    s = s.lower()\n",
    "    # Replace special characters with spaces\n",
    "    s = s.replace('-', ' ')\n",
    "    s = s.replace('_', ' ')\n",
    "    s = s.replace(',', '. ')\n",
    "    # Replace not contraction with not\n",
    "    s = s.replace(\"'nt\", \" not\")\n",
    "    s = s.replace(\"n't\", \" not\")\n",
    "    return(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_topics(topic_vectorizer, terms, n_terms=15, word_cloud=False, mask=None):\n",
    "    for topic_idx, topic in enumerate(topic_vectorizer):\n",
    "        message = \"Topic #%d: \" %(topic_idx+1)\n",
    "        print(message)\n",
    "        abs_topic = abs(topic)\n",
    "        topic_terms_sorted = \\\n",
    "        [[terms[i], topic[i]] \\\n",
    "            for i in abs_topic.argsort()[:-n_terms - 1:-1]]\n",
    "        k = 5\n",
    "        n = int(n_terms/k)\n",
    "        m = n_terms - k*n\n",
    "        for j in range(n):\n",
    "            l = k*j\n",
    "            message = ''\n",
    "            for i in range(k):\n",
    "                if topic_terms_sorted[i+l][1]>0:\n",
    "                    word = \"+\"+topic_terms_sorted[i+l][0]\n",
    "                else:\n",
    "                    word = \"-\"+topic_terms_sorted[i+l][0]\n",
    "                message += '{:<15s}'.format(word)\n",
    "            print(message)\n",
    "        \n",
    "        if m> 0:\n",
    "            l = k*n\n",
    "            message = ''\n",
    "            for i in range(m):\n",
    "                if topic_terms_sorted[i+l][1]>0:\n",
    "                    word = \"+\"+topic_terms_sorted[i+l][0]\n",
    "                else:\n",
    "                    word = \"-\"+topic_terms_sorted[i+l][0]\n",
    "                message += '{:<15s}'.format(word)\n",
    "            print(message)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_dic(tf, terms, scores=None):\n",
    "    td = {}\n",
    "    for i in range(tf.shape[0]):\n",
    "    # Iterate over the terms with nonzero scores\n",
    "        term_list = tf[i].nonzero()[1]\n",
    "        if len(term_list)>0:\n",
    "            if scores==None:\n",
    "                for t in np.nditer(term_list):\n",
    "                    if td.get(terms[t]) == None:\n",
    "                        td[terms[t]] = tf[i,t]\n",
    "                    else:\n",
    "                        td[terms[t]] += tf[i,t]\n",
    "            else:\n",
    "                for t in np.nditer(term_list):\n",
    "                    score = scores.get(terms[t])\n",
    "                    if score != None:\n",
    "                    # Found Sentiment Word\n",
    "                        score_weight = abs(scores[terms[t]])\n",
    "                        if td.get(terms[t]) == None:\n",
    "                            td[terms[t]] = tf[i,t] * score_weight\n",
    "                        else:\n",
    "                            td[terms[t]] += tf[i,t] * score_weight\n",
    "    return td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews.....  5310\n",
      "Number of Terms.......  2849\n"
     ]
    }
   ],
   "source": [
    "n_reviews = len(df['description'])\n",
    "cv = CountVectorizer(max_df=0.7, min_df=4, max_features=None,\\\n",
    "analyzer=my_analyzer)\n",
    "tf = cv.fit_transform(df['description'])\n",
    "terms = cv.get_feature_names()\n",
    "print('{:.<22s}{:>6d}'.format(\"Number of Reviews\", n_reviews))\n",
    "print('{:.<22s}{:>6d}'.format(\"Number of Terms\", len(terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Corpus contains a total of  2849  unique terms.\n",
      "The total number of terms in the Corpus is 218543\n",
      "\n",
      "Terms with Highest Frequency:\n",
      "honda          6149.000000\n",
      "transmission   5288.000000\n",
      "vehicle        5100.000000\n",
      "car            4614.000000\n",
      "problem        2986.000000\n",
      "contact        2864.000000\n",
      "dealer         2728.000000\n",
      "failure        2344.000000\n",
      "drive          2287.000000\n",
      "light          2204.000000\n"
     ]
    }
   ],
   "source": [
    "td = term_dic(tf, terms)\n",
    "print(\"The Corpus contains a total of \", len(td), \" unique terms.\")\n",
    "print(\"The total number of terms in the Corpus is\", sum(td.values()))\n",
    "term_sums = tf.sum(axis=0)\n",
    "term_counts = []\n",
    "for i in range(len(terms)):\n",
    "    term_counts.append([terms[i], term_sums[0,i]])\n",
    "def sortSecond(e):\n",
    "    return e[1]\n",
    "term_counts.sort(key=sortSecond, reverse=True)\n",
    "print(\"\\nTerms with Highest Frequency:\")\n",
    "for i in range(10):\n",
    "    print('{:<15s}{:>5f}'.format(term_counts[i][0], term_counts[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conducting Term/Frequency Matrix using TF-IDF\n",
      "The Term/Frequency matrix has 5310  rows, and 2849  columns.\n",
      "The Term list has 2849  terms.\n",
      "\n",
      "Terms with Highest TF-IDF Scores:\n",
      "transmission   10184.37\n",
      "honda           9511.28\n",
      "car             9507.42\n",
      "vehicle         9219.45\n",
      "contact         7025.56\n",
      "problem         6343.92\n",
      "dealer          5582.90\n",
      "failure         5298.20\n",
      "light           5227.67\n",
      "would           4809.46\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConducting Term/Frequency Matrix using TF-IDF\")\n",
    "# Default for norm is 'l2', use norm=None to supress\n",
    "tfidf_vect = TfidfTransformer(norm=None, use_idf=True) #set norm=None\n",
    "# tf matrix is (n_reviews)x(m_features\n",
    "tf = tfidf_vect.fit_transform(tf)\n",
    "term_idf_sums = tf.sum(axis=0)\n",
    "term_idf_scores = []\n",
    "for i in range(len(terms)):\n",
    "    term_idf_scores.append([terms[i], term_idf_sums[0,i]])\n",
    "print(\"The Term/Frequency matrix has\", tf.shape[0], \" rows, and\",\\\n",
    "    tf.shape[1], \" columns.\")\n",
    "print(\"The Term list has\", len(terms), \" terms.\")\n",
    "term_idf_scores.sort(key=sortSecond, reverse=True)\n",
    "print(\"\\nTerms with Highest TF-IDF Scores:\")\n",
    "for i in range(10):\n",
    "    j = i\n",
    "    print('{:<15s}{:>8.2f}'.format(term_idf_scores[j][0], \\\n",
    "    term_idf_scores[j][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** GENERATED TOPICS **********\n",
      "Topic #1: \n",
      "+transmission  +car           +gear          +shift         +drive         \n",
      "+stop          +would         +engine        +start         +slip          \n",
      "+accelerate    +vehicle       +problem       +brake         +traffic       \n",
      "Topic #2: \n",
      "+contact       +vehicle       +failure       +mileage       +state         \n",
      "+own           +repair        +airbags       +manufacturer  +mph           \n",
      "+current       +driver        +deploy        +campaign      +side          \n",
      "Topic #3: \n",
      "+information   +please        +car           +honda         +recall        \n",
      "+assistance    +fuel          +break         +regard        +provide       \n",
      "+problem       +daughter      +gas           +complaint     +vehicle       \n",
      "Topic #4: \n",
      "+switch        +door          +honda         +air           +dealer        \n",
      "+transmission  +open          +converter     +torque        +motor         \n",
      "+crv           +work          +compressor    +odyssey       +recall        \n",
      "Topic #5: \n",
      "+light         +airbags       +sr            +come          +dealer        \n",
      "+honda         +problem       +part          +srs           +seat          \n",
      "+recall        +illuminate    +issue         +sensor        +system        \n",
      "Topic #6: \n",
      "+honda         +transmission  +car           +mile          +problem       \n",
      "+issue         +warranty      +tire          +recall        +year          \n",
      "+acura         +new           +tell          +pay           +replace       \n",
      "Topic #7: \n",
      "+front         +car           +oil           +brake         +noise         \n",
      "+right         +side          +rear          +wheel         +seat          \n",
      "+steer         +lock          +leak          +rust          +belt          \n",
      "   topic        T1        T2        T3        T4        T5        T6        T7\n",
      "0      5  0.001384  0.146098  0.001385  0.001383  0.001384  0.653960  0.194405\n",
      "1      2  0.004172  0.250106  0.480853  0.004159  0.004170  0.004171  0.252369\n",
      "2      5  0.003818  0.218703  0.003820  0.003824  0.003826  0.762194  0.003815\n",
      "3      5  0.003894  0.293202  0.003895  0.003899  0.003901  0.687318  0.003891\n",
      "4      1  0.142151  0.850098  0.001549  0.001549  0.001550  0.001549  0.001554\n",
      "                                         description  NhtsaID   Make   Model  \\\n",
      "0  CONSUMER STATES FIRESTONE TIRE (NO SIZE) EXPER...   560001  HONDA   CIVIC   \n",
      "1  THE VEHICLE EXPERIENCES EXCESSIVE VIBRATION OF...   561194  HONDA  ACCORD   \n",
      "2  CONSUMER IS NOT HAPPY WITH THE DEALER DURING R...   562006  HONDA   CIVIC   \n",
      "3  CONSUMER IS NOT HAPPY WITH THE DEALER DURING R...   562066  HONDA   CIVIC   \n",
      "4  CONSUMER STATES WHEN HE PROCEEDED TO BACK UP, ...   562091  HONDA  ACCORD   \n",
      "\n",
      "   Year State abs cruise crash  mph  mileage  \n",
      "0  2001    CA   N      N     N   31  85064.0  \n",
      "1  2001    CA   N      N     N   70  87186.0  \n",
      "2  2001    WV   N      N     N   31  81110.0  \n",
      "3  2001    WV   N      N     N   31  81110.0  \n",
      "4  2001    PA   N      N     N   24  96792.0  \n"
     ]
    }
   ],
   "source": [
    "n_topics = 7\n",
    "\n",
    "uv = LatentDirichletAllocation(n_components=n_topics, \\\n",
    "learning_method='online', random_state=12345)\n",
    "U = uv.fit_transform(tf)\n",
    "\n",
    "print(\"\\n********** GENERATED TOPICS **********\")\n",
    "\n",
    "display_topics(uv.components_, terms, n_terms=15)\n",
    "\n",
    "# Store topic selection for each doc in topics[]\n",
    "topics = [0] * n_reviews\n",
    "for i in range(n_reviews):\n",
    "    max = abs(U[i][0])\n",
    "    topics[i] = 0\n",
    "    for j in range(n_topics):\n",
    "        x = abs(U[i][j])\n",
    "        if x > max:\n",
    "            max = x\n",
    "            topics[i] = j\n",
    "U_rev_scores = []\n",
    "for i in range(n_reviews):\n",
    "    u = [0] * (n_topics+1)\n",
    "    u[0] = topics[i]\n",
    "    for j in range(n_topics):\n",
    "        u[j+1] = U[i][j]\n",
    "    U_rev_scores.append(u)\n",
    "rev_scores = U_rev_scores\n",
    "\n",
    "# Integrate Topic Scores into Main Data Frame (df)\n",
    "cols = [\"topic\"]\n",
    "for i in range(n_topics):\n",
    "    s = \"T\"+str(i+1)\n",
    "    cols.append(s)\n",
    "df_rev = pd.DataFrame.from_records(rev_scores, columns=cols)\n",
    "print(df_rev.head())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['description'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.join(df_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['description'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>NhtsaID</th>\n",
       "      <th>Make</th>\n",
       "      <th>Model</th>\n",
       "      <th>Year</th>\n",
       "      <th>State</th>\n",
       "      <th>abs</th>\n",
       "      <th>cruise</th>\n",
       "      <th>crash</th>\n",
       "      <th>mph</th>\n",
       "      <th>mileage</th>\n",
       "      <th>topic</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CONSUMER STATES FIRESTONE TIRE (NO SIZE) EXPER...</td>\n",
       "      <td>560001</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>CIVIC</td>\n",
       "      <td>2001</td>\n",
       "      <td>CA</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>31</td>\n",
       "      <td>85064.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.146098</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.653960</td>\n",
       "      <td>0.194405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE VEHICLE EXPERIENCES EXCESSIVE VIBRATION OF...</td>\n",
       "      <td>561194</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>ACCORD</td>\n",
       "      <td>2001</td>\n",
       "      <td>CA</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>70</td>\n",
       "      <td>87186.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.250106</td>\n",
       "      <td>0.480853</td>\n",
       "      <td>0.004159</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.252369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CONSUMER IS NOT HAPPY WITH THE DEALER DURING R...</td>\n",
       "      <td>562006</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>CIVIC</td>\n",
       "      <td>2001</td>\n",
       "      <td>WV</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>31</td>\n",
       "      <td>81110.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.218703</td>\n",
       "      <td>0.003820</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.762194</td>\n",
       "      <td>0.003815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CONSUMER IS NOT HAPPY WITH THE DEALER DURING R...</td>\n",
       "      <td>562066</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>CIVIC</td>\n",
       "      <td>2001</td>\n",
       "      <td>WV</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>31</td>\n",
       "      <td>81110.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.293202</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.003901</td>\n",
       "      <td>0.687318</td>\n",
       "      <td>0.003891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CONSUMER STATES WHEN HE PROCEEDED TO BACK UP, ...</td>\n",
       "      <td>562091</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>ACCORD</td>\n",
       "      <td>2001</td>\n",
       "      <td>PA</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>24</td>\n",
       "      <td>96792.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142151</td>\n",
       "      <td>0.850098</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.001554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  NhtsaID   Make   Model  \\\n",
       "0  CONSUMER STATES FIRESTONE TIRE (NO SIZE) EXPER...   560001  HONDA   CIVIC   \n",
       "1  THE VEHICLE EXPERIENCES EXCESSIVE VIBRATION OF...   561194  HONDA  ACCORD   \n",
       "2  CONSUMER IS NOT HAPPY WITH THE DEALER DURING R...   562006  HONDA   CIVIC   \n",
       "3  CONSUMER IS NOT HAPPY WITH THE DEALER DURING R...   562066  HONDA   CIVIC   \n",
       "4  CONSUMER STATES WHEN HE PROCEEDED TO BACK UP, ...   562091  HONDA  ACCORD   \n",
       "\n",
       "   Year State abs cruise crash  mph  mileage  topic        T1        T2  \\\n",
       "0  2001    CA   N      N     N   31  85064.0    5.0  0.001384  0.146098   \n",
       "1  2001    CA   N      N     N   70  87186.0    2.0  0.004172  0.250106   \n",
       "2  2001    WV   N      N     N   31  81110.0    5.0  0.003818  0.218703   \n",
       "3  2001    WV   N      N     N   31  81110.0    5.0  0.003894  0.293202   \n",
       "4  2001    PA   N      N     N   24  96792.0    1.0  0.142151  0.850098   \n",
       "\n",
       "         T3        T4        T5        T6        T7  \n",
       "0  0.001385  0.001383  0.001384  0.653960  0.194405  \n",
       "1  0.480853  0.004159  0.004170  0.004171  0.252369  \n",
       "2  0.003820  0.003824  0.003826  0.762194  0.003815  \n",
       "3  0.003895  0.003899  0.003901  0.687318  0.003891  \n",
       "4  0.001549  0.001549  0.001550  0.001549  0.001554  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TOPIC DISTRIBUTION\n",
      "TOPIC    N     PERCENT\n",
      "----------------------\n",
      "  1    1321     24.9%\n",
      "  2    1149     21.6%\n",
      "  3     250      4.7%\n",
      "  4     334      6.3%\n",
      "  5     699     13.2%\n",
      "  6    1107     20.8%\n",
      "  7     430      8.1%\n"
     ]
    }
   ],
   "source": [
    "print(\" TOPIC DISTRIBUTION\")\n",
    "print('{:<6s}{:>4s}{:>12s}'.format(\"TOPIC\", \"N\", \"PERCENT\"))\n",
    "print(\"----------------------\")\n",
    "topic_counts = df['topic'].value_counts(sort=False)\n",
    "for i in range(len(topic_counts)):\n",
    "    percent = 100*topic_counts[i]/n_reviews\n",
    "    print('{:>3d}{:>8d}{:>9.1f}%'.format((i+1), topic_counts[i], percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N    3878\n",
       "Y    1432\n",
       "Name: abs, dtype: int64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['abs'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews.....  5310\n",
      "Number of Terms.......131964\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_df=0.95, min_df=1, max_features=None, \\\n",
    "preprocessor=my_preprocessor, ngram_range=(1,2))\n",
    "tf = cv.fit_transform(df['description'])\n",
    "s_terms = cv.get_feature_names()\n",
    "n_reviews = tf.shape[0]\n",
    "n_terms = tf.shape[1]\n",
    "print('{:.<22s}{:>6d}'.format(\"Number of Reviews\", n_reviews))\n",
    "print('{:.<22s}{:>6d}'.format(\"Number of Terms\", n_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_score(tf,df, text): \n",
    "\n",
    "    \n",
    "    min_sentiment = +5\n",
    "    max_sentiment = -5\n",
    "    avg_sentiment, min, max = 0,0,0\n",
    "    min_list, max_list = [],[]\n",
    "    sentiment_score = [0]*n_reviews\n",
    "    for i in range(n_reviews):\n",
    "            # Iterate over the terms with nonzero scores\n",
    "                n_sw = 0\n",
    "                term_list = tf[i].nonzero()[1]\n",
    "                if len(term_list)>0:\n",
    "                    for t in np.nditer(term_list):\n",
    "                        score = sentiment_dic.get(s_terms[t])\n",
    "                        if score != None:\n",
    "                            sentiment_score[i] += score * tf[i,t]\n",
    "                            n_sw += tf[i,t]\n",
    "                if n_sw>0:\n",
    "                    sentiment_score[i] = sentiment_score[i]/n_sw\n",
    "                if sentiment_score[i]==max_sentiment and n_sw>3:\n",
    "                    max_list.append(i)\n",
    "                if sentiment_score[i]>max_sentiment and n_sw>3:\n",
    "                    max_sentiment=sentiment_score[i]\n",
    "                    max = i\n",
    "                    max_list = [i]\n",
    "                if sentiment_score[i]==min_sentiment and n_sw>3:\n",
    "                    min_list.append(i)\n",
    "                if sentiment_score[i]<min_sentiment and n_sw>3:\n",
    "                    min_sentiment=sentiment_score[i]\n",
    "                    min = i\n",
    "                    min_list = [i]\n",
    "                avg_sentiment += sentiment_score[i]\n",
    "    avg_sentiment = avg_sentiment/n_reviews\n",
    "    print( \"Overall Average Sentiment: \", avg_sentiment)\n",
    "    df['Sentiment_Score']= sentiment_score\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Average Sentiment:  -1.083618492\n"
     ]
    }
   ],
   "source": [
    "sent_score(tf,df,'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2002    3144\n",
       "2001    1775\n",
       "2003     391\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('C:\\Texas A&M Spring Semester\\STAT 656')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Since all these variables are not necessary we can drop them.\n",
    "df.drop(['State','NhtsaID','description'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Since many scikit learn methods will not accept string values \n",
    "##The following values should be converted to numeric\n",
    "cat_map={'Y':1, 'N':2}\n",
    "cat_map3={'HONDA':1, 'ACURA':2}\n",
    "cat_map4={'TL':1,'ODYSSEY':2,'CR-V':3,'CL':4,'CIVIC':5,'ACCORD':6}\n",
    "df['Model']=df['Model'].map(cat_map4)\n",
    "df['crash']=df['crash'].map(cat_map)\n",
    "df['cruise']=df['cruise'].map(cat_map)\n",
    "df['abs']=df['abs'].map(cat_map)\n",
    "df['Make']=df['Make'].map(cat_map3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attribute_map = {\n",
    "    'Year':[2,(2001,2002,2003),[0,0]],\n",
    "    'Make':[2,(1,2),[0,0]],\n",
    "    'Model':[2,(1,2,3,4,5,6),[0,0]],\n",
    "    'crash':[1,(1,2),[0,0]],\n",
    "    'cruise':[1,(1,2),[0,0]],\n",
    "    'abs':[1,(1,2),[0,0]],\n",
    "    'mileage':[0,(1, 200000),[0,0]],\n",
    "    'mph':[0,(0, 80),[0,0]],\n",
    "    'topic':[2,(0.0,1.0,2.0,3.0,4.0,5.0,6.0),[0,0]],\n",
    "    'T1':[0,(-1e+8,1e+8),[0,0]],\n",
    "    'T2':[0,(-1e+8,1e+8),[0,0]],\n",
    "    'T3':[0,(-1e+8,1e+8),[0,0]],\n",
    "    'T4':[0,(-1e+8,1e+8),[0,0]],\n",
    "    'T5':[0,(-1e+8,1e+8),[0,0]],\n",
    "    'T6':[0,(-1e+8,1e+8),[0,0]],\n",
    "    'T7':[0,(-1e+8,1e+8),[0,0]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observation in the new dataset are :- 5310\n"
     ]
    }
   ],
   "source": [
    "##Data Preprocessing Starts\n",
    "feature_names=np.asarray(df.columns)\n",
    "initial_missing=df.isnull().sum()\n",
    "print('The number of observation in the new dataset are :-',df.shape[0])\n",
    "#new_df.describe()\n",
    "# Initialize number missing in attribute_map\n",
    "for k,v in attribute_map.items():\n",
    "    for feature in feature_names:\n",
    "        if feature==k:\n",
    "            v[2][0] = initial_missing[feature]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing outliers and setting all outliers as missing value\n",
    "for i in (df.index):\n",
    "    # For each observations, Iterate over all attributes.\n",
    "    # k is the attributes name and v is its metadata\n",
    "    for k, v in attribute_map.items():\n",
    "        # Check if the data is missing\n",
    "             \n",
    "        if v[0]==0: # Interval Attribute\n",
    "            l_limit = v[1][0] # get lower limit from metadata\n",
    "            u_limit = v[1][1] # get upper limit from metadata\n",
    "            # If the observation is outside the limits, its an outlier\n",
    "            if df.loc[i, k]>u_limit or df.loc[i,k]<l_limit:\n",
    "                v[2][1] += 1        # Number of outliers in metadata\n",
    "                df.loc[i,k] = None  # Set outlier to missing\n",
    "                \n",
    "        else: # Categorical Attribute or Other\n",
    "            \n",
    "            in_cat = False\n",
    "            # Iterate over the allowed categories for this attribute\n",
    "            for cat in v[1]:\n",
    "                if df.loc[i,k]==cat: # Found the category, not outlier\n",
    "                    in_cat=True\n",
    "            if in_cat==False:  # Did not find this category in the metadata\n",
    "                df.loc[i,k] = None  # This data is not recognized, its an outlier\n",
    "                v[2][1] += 1        # Increment the outlier counter for this attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of missing values and outliers by attribute:\n",
      "Year:\t0 missing  0 outlier(s)\n",
      "Make:\t0 missing  0 outlier(s)\n",
      "Model:\t0 missing  0 outlier(s)\n",
      "crash:\t0 missing  0 outlier(s)\n",
      "cruise:\t0 missing  0 outlier(s)\n",
      "abs:\t0 missing  0 outlier(s)\n",
      "mileage:\t1 missing  87 outlier(s)\n",
      "mph:\t0 missing  1 outlier(s)\n",
      "topic:\t20 missing  20 outlier(s)\n",
      "T1:\t20 missing  0 outlier(s)\n",
      "T2:\t20 missing  0 outlier(s)\n",
      "T3:\t20 missing  0 outlier(s)\n",
      "T4:\t20 missing  0 outlier(s)\n",
      "T5:\t20 missing  0 outlier(s)\n",
      "T6:\t20 missing  0 outlier(s)\n",
      "T7:\t20 missing  0 outlier(s)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNumber of missing values and outliers by attribute:\")\n",
    "feature_names = np.array(df.columns.values)\n",
    "for k,v in attribute_map.items():\n",
    "    print(k+\":\\t%i missing\" %v[2][0]+ \"  %i outlier(s)\" %v[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each of these lists will contain the names of the attributes in their level\n",
    "interval_attributes = []\n",
    "nominal_attributes  = []\n",
    "binary_attributes   = []\n",
    "onehot_attributes   = []\n",
    "# Iterate over the data dictionary\n",
    "for k,v in attribute_map.items():\n",
    "    if v[0]==0:\n",
    "        interval_attributes.append(k)\n",
    "    else:\n",
    "        if v[0]==1:\n",
    "            binary_attributes.append(k)\n",
    "        else:\n",
    "            nominal_attributes.append(k)\n",
    "            for i in range(len(v[1])):\n",
    "                str = k+(\"%i\" %i)\n",
    "                onehot_attributes.append(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 9 Interval Attributes,  3 Binary, and 4 Nominal Attribute\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_interval = len(interval_attributes)\n",
    "n_binary   = len(binary_attributes)\n",
    "n_nominal  = len(nominal_attributes)\n",
    "n_onehot   = len(onehot_attributes)\n",
    "print(\"\\nFound %i Interval Attributes, \" %n_interval, \\\n",
    "      \"%i Binary,\" %n_binary,  \\\n",
    "      \"and %i Nominal Attribute\\n\" %n_nominal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##Filling missing values, Imputation of the dataframe\n",
    "# Assigning the nominal and binary data from the dataframe into a numpy array\n",
    "from sklearn import preprocessing\n",
    "#print(\"Original DataFrame:\\n\", df[0:5])\n",
    "# Assigning the interval data from the dataframe into a numpy array\n",
    "interval_data = df.as_matrix(columns=interval_attributes)\n",
    "# Creating the Imputer for the Interval Data\n",
    "interval_imputer = preprocessing.Imputer(strategy='mean')\n",
    "# Imputing the missing values in the Interval data\n",
    "imputed_interval_data = interval_imputer.fit_transform(interval_data)\n",
    "nominal_data = df.as_matrix(columns=nominal_attributes)\n",
    "binary_data  = df.as_matrix(columns=binary_attributes)\n",
    "# Creating Imputer for Categorical Data\n",
    "cat_imputer = preprocessing.Imputer(strategy='most_frequent')\n",
    "# Imputing the missing values in the Categorical Data\n",
    "imputed_nominal_data = cat_imputer.fit_transform(nominal_data)\n",
    "imputed_binary_data  = cat_imputer.fit_transform(binary_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mileage</th>\n",
       "      <th>mph</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>crash</th>\n",
       "      <th>cruise</th>\n",
       "      <th>abs</th>\n",
       "      <th>Year</th>\n",
       "      <th>Make</th>\n",
       "      <th>Model</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.00000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.00000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>84171.340291</td>\n",
       "      <td>29.29045</td>\n",
       "      <td>0.218114</td>\n",
       "      <td>0.184062</td>\n",
       "      <td>0.078529</td>\n",
       "      <td>0.091864</td>\n",
       "      <td>0.143753</td>\n",
       "      <td>0.182936</td>\n",
       "      <td>0.100743</td>\n",
       "      <td>1.892844</td>\n",
       "      <td>1.678343</td>\n",
       "      <td>1.730320</td>\n",
       "      <td>2001.73936</td>\n",
       "      <td>1.128249</td>\n",
       "      <td>4.157062</td>\n",
       "      <td>2.554049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>37632.903435</td>\n",
       "      <td>17.49206</td>\n",
       "      <td>0.266575</td>\n",
       "      <td>0.264190</td>\n",
       "      <td>0.139033</td>\n",
       "      <td>0.147152</td>\n",
       "      <td>0.229529</td>\n",
       "      <td>0.234870</td>\n",
       "      <td>0.170869</td>\n",
       "      <td>0.309341</td>\n",
       "      <td>0.467156</td>\n",
       "      <td>0.443835</td>\n",
       "      <td>0.58313</td>\n",
       "      <td>0.334398</td>\n",
       "      <td>1.833378</td>\n",
       "      <td>2.163760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2001.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>64597.500000</td>\n",
       "      <td>22.00000</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2001.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>85064.000000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>0.104214</td>\n",
       "      <td>0.025353</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>0.005951</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.065444</td>\n",
       "      <td>0.005513</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2002.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>101317.000000</td>\n",
       "      <td>37.00000</td>\n",
       "      <td>0.367987</td>\n",
       "      <td>0.315430</td>\n",
       "      <td>0.109041</td>\n",
       "      <td>0.130246</td>\n",
       "      <td>0.196972</td>\n",
       "      <td>0.313648</td>\n",
       "      <td>0.132063</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2002.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>80.00000</td>\n",
       "      <td>0.995414</td>\n",
       "      <td>0.995787</td>\n",
       "      <td>0.989362</td>\n",
       "      <td>0.992179</td>\n",
       "      <td>0.995268</td>\n",
       "      <td>0.992927</td>\n",
       "      <td>0.996419</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2003.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mileage         mph           T1           T2           T3  \\\n",
       "count    5310.000000  5310.00000  5310.000000  5310.000000  5310.000000   \n",
       "mean    84171.340291    29.29045     0.218114     0.184062     0.078529   \n",
       "std     37632.903435    17.49206     0.266575     0.264190     0.139033   \n",
       "min         1.000000     0.00000     0.000177     0.000163     0.000185   \n",
       "25%     64597.500000    22.00000     0.001850     0.001287     0.001235   \n",
       "50%     85064.000000    30.00000     0.104214     0.025353     0.003501   \n",
       "75%    101317.000000    37.00000     0.367987     0.315430     0.109041   \n",
       "max    200000.000000    80.00000     0.995414     0.995787     0.989362   \n",
       "\n",
       "                T4           T5           T6           T7        crash  \\\n",
       "count  5310.000000  5310.000000  5310.000000  5310.000000  5310.000000   \n",
       "mean      0.091864     0.143753     0.182936     0.100743     1.892844   \n",
       "std       0.147152     0.229529     0.234870     0.170869     0.309341   \n",
       "min       0.000187     0.000185     0.000202     0.000187     1.000000   \n",
       "25%       0.001404     0.001322     0.001652     0.001338     2.000000   \n",
       "50%       0.005951     0.007670     0.065444     0.005513     2.000000   \n",
       "75%       0.130246     0.196972     0.313648     0.132063     2.000000   \n",
       "max       0.992179     0.995268     0.992927     0.996419     2.000000   \n",
       "\n",
       "            cruise          abs        Year         Make        Model  \\\n",
       "count  5310.000000  5310.000000  5310.00000  5310.000000  5310.000000   \n",
       "mean      1.678343     1.730320  2001.73936     1.128249     4.157062   \n",
       "std       0.467156     0.443835     0.58313     0.334398     1.833378   \n",
       "min       1.000000     1.000000  2001.00000     1.000000     1.000000   \n",
       "25%       1.000000     1.000000  2001.00000     1.000000     2.000000   \n",
       "50%       2.000000     2.000000  2002.00000     1.000000     5.000000   \n",
       "75%       2.000000     2.000000  2002.00000     1.000000     6.000000   \n",
       "max       2.000000     2.000000  2003.00000     2.000000     6.000000   \n",
       "\n",
       "             topic  \n",
       "count  5310.000000  \n",
       "mean      2.554049  \n",
       "std       2.163760  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       2.000000  \n",
       "75%       5.000000  \n",
       "max       6.000000  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bring Interval and Categorial Data Together\n",
    "# The Imputed Data\n",
    "data_array= np.hstack((imputed_interval_data, imputed_binary_data, \\\n",
    "                       imputed_nominal_data))\n",
    "col = []\n",
    "for i in range(n_interval):\n",
    "    col.append(interval_attributes[i])\n",
    "for i in range(n_binary):\n",
    "    col.append(binary_attributes[i])\n",
    "for i in range(n_nominal):\n",
    "    col.append(nominal_attributes[i])\n",
    "df_imputed = pd.DataFrame(data_array,columns=col)\n",
    "#print(\"\\nImputed DataFrame:\\n\", df_imputed[0:5])\n",
    "df_imputed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Nominal Attributes\n",
    "##Creating an instance of the OneHotEncoder & Selecting Attributes\n",
    "onehot = preprocessing.OneHotEncoder()\n",
    "hot_array = onehot.fit_transform(imputed_nominal_data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mileage', 'mph', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'crash',\n",
       "       'cruise', 'abs', 'Year0', 'Year1', 'Year2', 'Make0', 'Make1', 'Model0',\n",
       "       'Model1', 'Model2', 'Model3', 'Model4', 'Model5', 'topic0', 'topic1',\n",
       "       'topic2', 'topic3', 'topic4', 'topic5', 'topic6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I have not scaled the interval data as the range for interval attributes is not that big.\n",
    "# The Imputed and Encoded Data\n",
    "\n",
    "data_array = np.hstack((imputed_interval_data, imputed_binary_data, hot_array))\n",
    "#col = (interval_attributes, cat_attributes)\n",
    "col = []\n",
    "for i in range(n_interval):\n",
    "    col.append(interval_attributes[i])\n",
    "for i in range(n_binary):\n",
    "    col.append(binary_attributes[i])\n",
    "for i in range(n_onehot):\n",
    "    col.append(onehot_attributes[i])\n",
    "df_imputed_scaled = pd.DataFrame(data_array,columns=col)\n",
    "df_imputed_scaled.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mileage</th>\n",
       "      <th>mph</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>crash</th>\n",
       "      <th>...</th>\n",
       "      <th>Model3</th>\n",
       "      <th>Model4</th>\n",
       "      <th>Model5</th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.00000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>84171.340291</td>\n",
       "      <td>29.29045</td>\n",
       "      <td>0.218114</td>\n",
       "      <td>0.184062</td>\n",
       "      <td>0.078529</td>\n",
       "      <td>0.091864</td>\n",
       "      <td>0.143753</td>\n",
       "      <td>0.182936</td>\n",
       "      <td>0.100743</td>\n",
       "      <td>1.892844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010734</td>\n",
       "      <td>0.299058</td>\n",
       "      <td>0.317137</td>\n",
       "      <td>0.252542</td>\n",
       "      <td>0.216384</td>\n",
       "      <td>0.047081</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.131638</td>\n",
       "      <td>0.208475</td>\n",
       "      <td>0.080979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>37632.903435</td>\n",
       "      <td>17.49206</td>\n",
       "      <td>0.266575</td>\n",
       "      <td>0.264190</td>\n",
       "      <td>0.139033</td>\n",
       "      <td>0.147152</td>\n",
       "      <td>0.229529</td>\n",
       "      <td>0.234870</td>\n",
       "      <td>0.170869</td>\n",
       "      <td>0.309341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103059</td>\n",
       "      <td>0.457889</td>\n",
       "      <td>0.465405</td>\n",
       "      <td>0.434512</td>\n",
       "      <td>0.411818</td>\n",
       "      <td>0.211832</td>\n",
       "      <td>0.242806</td>\n",
       "      <td>0.338129</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.272829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>64597.500000</td>\n",
       "      <td>22.00000</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>85064.000000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>0.104214</td>\n",
       "      <td>0.025353</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>0.005951</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.065444</td>\n",
       "      <td>0.005513</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>101317.000000</td>\n",
       "      <td>37.00000</td>\n",
       "      <td>0.367987</td>\n",
       "      <td>0.315430</td>\n",
       "      <td>0.109041</td>\n",
       "      <td>0.130246</td>\n",
       "      <td>0.196972</td>\n",
       "      <td>0.313648</td>\n",
       "      <td>0.132063</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>80.00000</td>\n",
       "      <td>0.995414</td>\n",
       "      <td>0.995787</td>\n",
       "      <td>0.989362</td>\n",
       "      <td>0.992179</td>\n",
       "      <td>0.995268</td>\n",
       "      <td>0.992927</td>\n",
       "      <td>0.996419</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             mileage         mph           T1           T2           T3  \\\n",
       "count    5310.000000  5310.00000  5310.000000  5310.000000  5310.000000   \n",
       "mean    84171.340291    29.29045     0.218114     0.184062     0.078529   \n",
       "std     37632.903435    17.49206     0.266575     0.264190     0.139033   \n",
       "min         1.000000     0.00000     0.000177     0.000163     0.000185   \n",
       "25%     64597.500000    22.00000     0.001850     0.001287     0.001235   \n",
       "50%     85064.000000    30.00000     0.104214     0.025353     0.003501   \n",
       "75%    101317.000000    37.00000     0.367987     0.315430     0.109041   \n",
       "max    200000.000000    80.00000     0.995414     0.995787     0.989362   \n",
       "\n",
       "                T4           T5           T6           T7        crash  \\\n",
       "count  5310.000000  5310.000000  5310.000000  5310.000000  5310.000000   \n",
       "mean      0.091864     0.143753     0.182936     0.100743     1.892844   \n",
       "std       0.147152     0.229529     0.234870     0.170869     0.309341   \n",
       "min       0.000187     0.000185     0.000202     0.000187     1.000000   \n",
       "25%       0.001404     0.001322     0.001652     0.001338     2.000000   \n",
       "50%       0.005951     0.007670     0.065444     0.005513     2.000000   \n",
       "75%       0.130246     0.196972     0.313648     0.132063     2.000000   \n",
       "max       0.992179     0.995268     0.992927     0.996419     2.000000   \n",
       "\n",
       "          ...            Model3       Model4       Model5       topic0  \\\n",
       "count     ...       5310.000000  5310.000000  5310.000000  5310.000000   \n",
       "mean      ...          0.010734     0.299058     0.317137     0.252542   \n",
       "std       ...          0.103059     0.457889     0.465405     0.434512   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "75%       ...          0.000000     1.000000     1.000000     1.000000   \n",
       "max       ...          1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "            topic1       topic2       topic3       topic4       topic5  \\\n",
       "count  5310.000000  5310.000000  5310.000000  5310.000000  5310.000000   \n",
       "mean      0.216384     0.047081     0.062900     0.131638     0.208475   \n",
       "std       0.411818     0.211832     0.242806     0.338129     0.406256   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "            topic6  \n",
       "count  5310.000000  \n",
       "mean      0.080979  \n",
       "std       0.272829  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imputed_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "varlist = ['crash', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7']\n",
    "X = df_imputed_scaled.drop(varlist, axis=1)\n",
    "y = df_imputed_scaled['crash'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_depth= 5\n",
      "Metric.......  Mean    Std. Dev.\n",
      "accuracy..... 0.9073    0.0104\n",
      "recall....... 0.2161    0.0743\n",
      "precision.... 0.7955    0.1843\n",
      "f1........... 0.3272    0.0919\n",
      "For max_depth= 6\n",
      "Metric.......  Mean    Std. Dev.\n",
      "accuracy..... 0.9090    0.0063\n",
      "recall....... 0.2548    0.0848\n",
      "precision.... 0.7466    0.1198\n",
      "f1........... 0.3665    0.0886\n",
      "For max_depth= 7\n",
      "Metric.......  Mean    Std. Dev.\n",
      "accuracy..... 0.9055    0.0070\n",
      "recall....... 0.2881    0.1071\n",
      "precision.... 0.6625    0.0902\n",
      "f1........... 0.3822    0.0983\n",
      "For max_depth= 8\n",
      "Metric.......  Mean    Std. Dev.\n",
      "accuracy..... 0.8987    0.0074\n",
      "recall....... 0.3197    0.1152\n",
      "precision.... 0.5577    0.0788\n",
      "f1........... 0.3865    0.0874\n",
      "For max_depth= 10\n",
      "Metric.......  Mean    Std. Dev.\n",
      "accuracy..... 0.8917    0.0118\n",
      "recall....... 0.3937    0.1293\n",
      "precision.... 0.4889    0.0792\n",
      "f1........... 0.4300    0.0935\n",
      "For max_depth= 12\n",
      "Metric.......  Mean    Std. Dev.\n",
      "accuracy..... 0.8863    0.0151\n",
      "recall....... 0.4025    0.1390\n",
      "precision.... 0.4628    0.0854\n",
      "f1........... 0.4212    0.1020\n",
      "For max_depth= 15\n",
      "Metric.......  Mean    Std. Dev.\n",
      "accuracy..... 0.8831    0.0159\n",
      "recall....... 0.4078    0.1371\n",
      "precision.... 0.4518    0.0874\n",
      "f1........... 0.4145    0.0959\n",
      "For max_depth= 20\n",
      "Metric.......  Mean    Std. Dev.\n",
      "accuracy..... 0.8825    0.0157\n",
      "recall....... 0.4078    0.1377\n",
      "precision.... 0.4479    0.0912\n",
      "f1........... 0.4167    0.0942\n",
      "For max_depth= 25\n",
      "Metric.......  Mean    Std. Dev.\n",
      "accuracy..... 0.8823    0.0148\n",
      "recall....... 0.4095    0.1387\n",
      "precision.... 0.4549    0.0829\n",
      "f1........... 0.4185    0.0991\n"
     ]
    }
   ],
   "source": [
    "max_depth=[5,6,7,8,10,12,15,20,25]\n",
    "for i in max_depth:\n",
    "    dtc = DecisionTreeClassifier(criterion='gini', max_depth=i, \\\n",
    "    min_samples_split=5, min_samples_leaf=5)\n",
    "    dtc = dtc.fit(X,y)\n",
    "    score_list = ['accuracy', 'recall', 'precision', 'f1']\n",
    "    mean_score = []\n",
    "    std_score = []\n",
    "    print(\"For max_depth=\",i)\n",
    "    print(\"{:.<13s}{:>6s}{:>13s}\".format(\"Metric\", \"Mean\", \"Std. Dev.\"))\n",
    "    for s in score_list:\n",
    "        dtc_10 = cross_val_score(dtc, X, y, scoring=s, cv=10)\n",
    "        mean = dtc_10.mean()\n",
    "        std = dtc_10.std()\n",
    "        mean_score.append(mean)\n",
    "        std_score.append(std)\n",
    "        print(\"{:.<13s}{:>7.4f}{:>10.4f}\".format(s, mean, std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth for the Decision Tree is 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Max depth for the Decision Tree is 6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(X,y,test_size = 0.3, random_state=7)\n",
    "score_list = ['accuracy', 'recall', 'precision', 'f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table of the metrics for 70/30 split\n",
      "\n",
      "\n",
      "Model Metrics..........       Training     Validation\n",
      "Observations...........           3717           1593\n",
      "Features...............             22             22\n",
      "Maximum Tree Depth.....              6              6\n",
      "Minimum Leaf Size......              5              5\n",
      "Minimum split Size.....              5              5\n",
      "Mean Absolute Error....         0.7402         0.7211\n",
      "Avg Squared Error......         0.6303         0.6116\n",
      "Accuracy...............         0.7659         0.7420\n",
      "Precision..............         0.3003         0.2530\n",
      "Recall (Sensitivity)...         0.8596         0.7791\n",
      "F1-score...............         0.4452         0.3820\n",
      "MISC (Misclassification)...      23.4%          25.8%\n",
      "     class 1...............      14.0%          22.1%\n",
      "     class 2...............      24.6%          26.2%\n",
      "\n",
      "\n",
      "Training\n",
      "Confusion Matrix  Class 1   Class 2  \n",
      "Class 1.....       349        57\n",
      "Class 2.....       813      2498\n",
      "\n",
      "\n",
      "Validation\n",
      "Confusion Matrix  Class 1   Class 2  \n",
      "Class 1.....       127        36\n",
      "Class 2.....       375      1055\n"
     ]
    }
   ],
   "source": [
    "dtc_train = DecisionTreeClassifier(criterion='gini', max_depth=6, \\\n",
    "min_samples_split=5, min_samples_leaf=5,\n",
    "class_weight='balanced').fit(X_train, y_train)\n",
    "print(\"\\nTable of the metrics for 70/30 split\")\n",
    "DecisionTree.display_binary_split_metrics(dtc_train, X_train,y_train,X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-250-6f7ade5cf11c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdtc_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-250-6f7ade5cf11c>\u001b[0m in \u001b[0;36mdtc_graph\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdtc_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     dot_data = tree.export_graphviz(dtc_train, out_file=None,\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mfeature_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mclass_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfilled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrounded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "def dtc_graph():\n",
    "    dot_data = tree.export_graphviz(dtc_train, out_file=None,\n",
    "    feature_names=list(X.columns),\n",
    "    class_names=['0','1'],\n",
    "    filled=True, rounded=True,\n",
    "    special_characters=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    return(graph)\n",
    "dtc_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Splitting data\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X,y,test_size = 0.3, random_state=7)\n",
    "score_list = ['accuracy', 'recall', 'precision', 'f1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from newsapi import NewsApiClient # Needed for using API Feed\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agency_urls = {\n",
    "'huffington': 'http://huffingtonpost.com',\n",
    "'reuters': 'http://www.reuters.com',\n",
    "'cbs-news': 'http://www.cbsnews.com',\n",
    "'usa-today': 'http://usatoday.com',\n",
    "'cnn': 'http://cnn.com',\n",
    "'npr': 'http://www.npr.org',\n",
    "'wsj': 'http://wsj.com',\n",
    "'fox': 'http://www.foxnews.com',\n",
    "'abc': 'http://abc.com',\n",
    "'abc-news': 'http://abcnews.com',\n",
    "'abcgonews': 'http://abcnews.go.com',\n",
    "'nyt': 'http://nytimes.com',\n",
    "'washington-post': 'http://washingtonpost.com',\n",
    "'us-news': 'http://www.usnews.com',\n",
    "'msn': 'http://msn.com',\n",
    "'pbs': 'http://www.pbs.org',\n",
    "'nbc-news': 'http://www.nbcnews.com',\n",
    "'enquirer': 'http://www.nationalenquirer.com',\n",
    "'la-times': 'http://www.latimes.com'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_html(html):\n",
    "# First we remove inline JavaScript/CSS:\n",
    "    pg = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \"\", html.strip())\n",
    "    # Then we remove html comments. This has to be done before removing regular\n",
    "    # tags since comments can contain '>' characters.\n",
    "    pg = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \"\", pg)\n",
    "    # Next we can remove the remaining tags:\n",
    "    pg = re.sub(r\"(?s)<.*?>\", \" \", pg)\n",
    "    # Finally, we deal with whitespace\n",
    "    pg = re.sub(r\"&nbsp;\", \" \", pg)\n",
    "    pg = re.sub(r\"&rsquo;\", \"'\", pg)\n",
    "    pg = re.sub(r\"&ldquo;\", '\"', pg)\n",
    "    pg = re.sub(r\"&rdquo;\", '\"', pg)\n",
    "    pg = re.sub(r\"\\n\", \" \", pg)\n",
    "    pg = re.sub(r\"\\t\", \" \", pg)\n",
    "    pg = re.sub(r\" \", \" \", pg)\n",
    "    pg = re.sub(r\" \", \" \", pg)\n",
    "    pg = re.sub(r\" \", \" \", pg)\n",
    "    return pg.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newsapi_get_urls(search_words, agency_urls):\n",
    "    if len(search_words)==0 or agency_urls==None:\n",
    "        return None\n",
    "    print(\"Searching agencies for pages containing:\", search_words)\n",
    "    # This is my API key, each user must request their own\n",
    "    # API key from https://newsapi.org/account\n",
    "    api = NewsApiClient(api_key='6f174feb5d05447d920d538d45718afa')\n",
    "    api_urls = []\n",
    "    # Iterate over agencies and search words to pull more url's\n",
    "    # Limited to 1,000 requests/day - Likely to be exceeded\n",
    "    for agency in agency_urls:\n",
    "        domain = agency_urls[agency].replace(\"http://\", \"\")\n",
    "        print(agency, domain)\n",
    "        for word in search_words:\n",
    "            try:\n",
    "                articles = api.get_everything(q=word, language='en',\\\n",
    "                sources=agency, domains=domain)\n",
    "            except:\n",
    "                print(\"--->Unable to pull news from:\", agency, \"for\", word)\n",
    "                continue\n",
    "# Pull the URL from these articles (limited to 20)\n",
    "            d = articles['articles']\n",
    "            for i in range(len(d)):\n",
    "                url = d[i]['url']\n",
    "                api_urls.append([agency, word, url])\n",
    "\n",
    "\n",
    "    df_urls = pd.DataFrame(api_urls, columns=['agency', 'word', 'url'])\n",
    "    n_total = len(df_urls)\n",
    "    # Remove duplicates\n",
    "    df_urls = df_urls.drop_duplicates('url')\n",
    "    n_unique = len(df_urls)\n",
    "    print(\"\\nFound a total of\", n_total, \" URLs, of which\", n_unique,\\\n",
    "    \" were unique.\")\n",
    "    return df_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def request_pages(df_urls):\n",
    "    web_pages = []\n",
    "    for i in range(len(df_urls)):\n",
    "        u = df_urls.iloc[i]\n",
    "        url = u[2]\n",
    "        short_url = url[0:50]\n",
    "        short_url = short_url.replace(\"https//\", \"\")\n",
    "        short_url = short_url.replace(\"http//\", \"\")\n",
    "        n = 0\n",
    "        # Allow for a maximum of 5 download failures\n",
    "        stop_sec=3 # Initial max wait time in seconds\n",
    "        while n<3:\n",
    "            try:\n",
    "                r = requests.get(url, timeout=(stop_sec))\n",
    "                if r.status_code == 408:\n",
    "                    print(\"-->HTML ERROR 408\", short_url)\n",
    "                    raise ValueError()\n",
    "                if r.status_code == 200:\n",
    "                    print(\"Obtained: \"+short_url)\n",
    "                else:\n",
    "                    print(\"-->Web page: \"+short_url+\" status code:\", \\\n",
    "                    r.status_code)\n",
    "                n=99\n",
    "                continue # Skip this pag\n",
    "            except:\n",
    "                n += 1\n",
    "                # Timeout waiting for download\n",
    "                t0 = time()\n",
    "                tlapse = 0\n",
    "                print(\"Waiting\", stop_sec, \"sec\")\n",
    "                while tlapse<stop_sec:\n",
    "                    tlapse = time()-t0\n",
    "        if n != 99:\n",
    "            # download failed skip this page\n",
    "            continue\n",
    "            # Page obtained successfully\n",
    "        html_page = r.text\n",
    "        page_text = clean_html(html_page)\n",
    "        web_pages.append([url, page_text])\n",
    "    df_www = pd.DataFrame(web_pages, columns=['url', 'text'])\n",
    "    n_total = len(df_urls)\n",
    "    # Remove duplicates\n",
    "    df_www = df_www.drop_duplicates('url')\n",
    "    n_unique = len(df_urls)\n",
    "    print(\"Found a total of\", n_total, \" web pages, of which\", n_unique,\\\n",
    "    \" were unique.\")\n",
    "    return df_www"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching agencies for pages containing: ['takata']\n",
      "huffington huffingtonpost.com\n",
      "reuters www.reuters.com\n",
      "cbs-news www.cbsnews.com\n",
      "usa-today usatoday.com\n",
      "cnn cnn.com\n",
      "npr www.npr.org\n",
      "wsj wsj.com\n",
      "fox www.foxnews.com\n",
      "abc abc.com\n",
      "abc-news abcnews.com\n",
      "abcgonews abcnews.go.com\n",
      "nyt nytimes.com\n",
      "washington-post washingtonpost.com\n",
      "us-news www.usnews.com\n",
      "msn msn.com\n",
      "pbs www.pbs.org\n",
      "nbc-news www.nbcnews.com\n",
      "enquirer www.nationalenquirer.com\n",
      "la-times www.latimes.com\n",
      "\n",
      "Found a total of 65  URLs, of which 62  were unique.\n",
      "Total Articles: 62\n"
     ]
    }
   ],
   "source": [
    "search_words = ['takata']\n",
    "df_urls = newsapi_get_urls(search_words, agency_urls)\n",
    "print(\"Total Articles:\", df_urls.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained: https://www.reuters.com/article/us-autos-takata/ho\n",
      "Obtained: https://in.reuters.com/article/autos-takata/honda-\n",
      "Obtained: https://www.reuters.com/article/us-autos-takata/u-\n",
      "Obtained: https://www.reuters.com/article/us-takata-whistleb\n",
      "Obtained: https://uk.reuters.com/article/uk-autos-takata/hon\n",
      "Obtained: https://www.reuters.com/article/us-takata-pricefix\n",
      "Obtained: https://www.reuters.com/article/us-autos-takata/se\n",
      "Obtained: https://ca.reuters.com/article/businessNews/idCAKC\n",
      "Obtained: https://www.reuters.com/article/us-autos-takata/au\n",
      "Obtained: https://in.reuters.com/article/autos-takata/automa\n",
      "Obtained: https://www.reuters.com/article/us-takata-bankrupt\n",
      "Obtained: https://www.reuters.com/article/us-takata-bankrupt\n",
      "Obtained: https://in.reuters.com/article/takata-bankruptcy-s\n",
      "Obtained: https://www.reuters.com/article/us-takata-bankrupt\n",
      "Obtained: https://www.reuters.com/article/us-takata-sale-key\n",
      "Obtained: https://ca.reuters.com/article/businessNews/idCAKC\n",
      "Obtained: https://www.reuters.com/article/us-takata-bankrupt\n",
      "Obtained: https://www.reuters.com/article/takata-pricefixing\n",
      "Obtained: https://www.reuters.com/article/takata-bankruptcy-\n",
      "Obtained: https://www.usatoday.com/story/money/cars/2018/04/\n",
      "Obtained: https://www.usatoday.com/story/money/cars/2018/02/\n",
      "Obtained: https://www.usatoday.com/story/money/cars/2018/02/\n",
      "Obtained: https://www.usatoday.com/story/money/cars/2018/01/\n",
      "Obtained: http://money.cnn.com/2018/02/27/news/companies/tak\n",
      "Obtained: http://money.cnn.com/2018/03/19/news/companies/hyu\n",
      "Obtained: https://www.wsj.com/articles/takata-whistleblower-\n",
      "Obtained: https://www.wsj.com/articles/takata-settles-with-d\n",
      "Obtained: https://www.wsj.com/articles/takata-settles-joint-\n",
      "Obtained: https://www.wsj.com/articles/regulator-car-executi\n",
      "Obtained: https://www.wsj.com/articles/more-auto-makers-sued\n",
      "Obtained: https://www.wsj.com/articles/senators-press-car-ex\n",
      "Obtained: https://www.wsj.com/articles/u-s-investigates-fail\n",
      "Obtained: https://www.wsj.com/articles/companies-everywhere-\n",
      "Obtained: http://abcnews.go.com/Business/wireStory/takata-ac\n",
      "Obtained: http://abcnews.go.com/International/wireStory/taka\n",
      "Obtained: http://abcnews.go.com/International/wireStory/taka\n",
      "Obtained: http://abcnews.go.com/International/wireStory/judg\n",
      "Obtained: http://abcnews.go.com/Business/wireStory/states-fo\n",
      "Obtained: http://abcnews.go.com/International/wireStory/zeal\n",
      "Obtained: http://abcnews.go.com/Business/wireStory/air-bag-d\n",
      "Obtained: http://abcnews.go.com/Business/wireStory/lawsuits-\n",
      "Obtained: http://abcnews.go.com/Business/wireStory/3rd-time-\n",
      "Obtained: http://abcnews.go.com/International/wireStory/aust\n",
      "Obtained: http://abcnews.go.com/Business/wireStory/honda-fau\n",
      "Obtained: http://abcnews.go.com/Business/wireStory/hondas-pr\n",
      "Obtained: http://abcnews.go.com/Business/wireStory/business-\n",
      "Obtained: http://abcnews.go.com/Business/wireStory/business-\n",
      "Obtained: http://abcnews.go.com/Business/wireStory/business-\n",
      "Obtained: https://www.nytimes.com/2018/02/22/business/takata\n",
      "Waiting 3 sec\n",
      "Obtained: https://www.nytimes.com/2018/02/11/business/takata\n",
      "Obtained: https://www.nytimes.com/2018/02/28/briefing/xi-jin\n",
      "Waiting 3 sec\n",
      "Obtained: https://www.nytimes.com/2018/02/23/business/dealbo\n",
      "-->Web page: https://www.washingtonpost.com/world/asia_pacific/ status code: 404\n",
      "-->Web page: https://www.washingtonpost.com/world/asia_pacific/ status code: 404\n",
      "-->Web page: https://www.washingtonpost.com/world/new-zealand-r status code: 404\n",
      "-->Web page: https://www.washingtonpost.com/world/australia-iss status code: 404\n",
      "-->Web page: https://www.washingtonpost.com/world/australia-iss status code: 404\n",
      "Obtained: https://www.washingtonpost.com/world/asia_pacific/\n",
      "Obtained: https://www.washingtonpost.com/news/the-switch/wp/\n",
      "Obtained: https://www.washingtonpost.com/business/economy/am\n",
      "Obtained: https://www.washingtonpost.com/news/dr-gridlock/wp\n",
      "Obtained: https://www.nbcnews.com/news/us-news/hyundai-kia-u\n",
      "Found a total of 62  web pages, of which 62  were unique.\n"
     ]
    }
   ],
   "source": [
    "df_www = request_pages(df_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_www= df_www.drop_duplicates('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_www.to_csv('df_www.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_www['text']= df_www['text'].str.replace(\"air bag\", \"airbag\")\n",
    "df_www['text']= df_www['text'].str.replace(\"air bags\", \"airbag\")\n",
    "df_www['text']= df_www['text'].str.replace(\"airbags\", \"airbag\")\n",
    "df_www['text']= df_www['text'].str.replace(\"Air bag\", \"airbag\")\n",
    "df_www['text']= df_www['text'].str.replace(\"Air bags\", \"airbag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reuters.com/article/us-autos-takat...</td>\n",
       "      <td>Honda, Ford to testify at U.S. Senate Takata h...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.998709</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://in.reuters.com/article/autos-takata/ho...</td>\n",
       "      <td>Honda, Ford to testify at U.S. Senate Takata h...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.998717</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.161290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.reuters.com/article/us-autos-takat...</td>\n",
       "      <td>U.S. senators call new hearing on Takata auto ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.998568</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.reuters.com/article/us-takata-whis...</td>\n",
       "      <td>Takata whistleblowers to share $1.7 million aw...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.697399</td>\n",
       "      <td>0.301479</td>\n",
       "      <td>0.141304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://uk.reuters.com/article/uk-autos-takata...</td>\n",
       "      <td>Honda, Ford to testify at U.S. Senate Takata h...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.998735</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                url  \\\n",
       "0           0  https://www.reuters.com/article/us-autos-takat...   \n",
       "1           1  https://in.reuters.com/article/autos-takata/ho...   \n",
       "2           2  https://www.reuters.com/article/us-autos-takat...   \n",
       "3           3  https://www.reuters.com/article/us-takata-whis...   \n",
       "4           4  https://uk.reuters.com/article/uk-autos-takata...   \n",
       "\n",
       "                                                text  topic        T1  \\\n",
       "0  Honda, Ford to testify at U.S. Senate Takata h...      5  0.000215   \n",
       "1  Honda, Ford to testify at U.S. Senate Takata h...      5  0.000214   \n",
       "2  U.S. senators call new hearing on Takata auto ...      5  0.000238   \n",
       "3  Takata whistleblowers to share $1.7 million aw...      5  0.000224   \n",
       "4  Honda, Ford to testify at U.S. Senate Takata h...      5  0.000211   \n",
       "\n",
       "         T2        T3        T4        T5        T6        T7  Sentiment_Score  \n",
       "0  0.000215  0.000215  0.000215  0.000215  0.998709  0.000215         0.000000  \n",
       "1  0.000214  0.000214  0.000214  0.000214  0.998717  0.000214         0.161290  \n",
       "2  0.000239  0.000239  0.000238  0.000239  0.998568  0.000239        -0.064516  \n",
       "3  0.000224  0.000225  0.000224  0.000225  0.697399  0.301479         0.141304  \n",
       "4  0.000211  0.000211  0.000211  0.000211  0.998735  0.000211         0.100000  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_www.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.reuters.com/article/us-autos-takata/honda-ford-to-testif      4863 Characters\n",
      "in.reuters.com/article/autos-takata/honda-ford-to-testify-at      4873 Characters\n",
      "www.reuters.com/article/us-autos-takata/u-s-senators-call-ne      4790 Characters\n",
      "www.reuters.com/article/us-takata-whistleblowers/takata-whis      5669 Characters\n",
      "uk.reuters.com/article/uk-autos-takata/honda-ford-to-testify      5006 Characters\n",
      "www.reuters.com/article/us-takata-pricefixing/south-africa-a      3129 Characters\n",
      "www.reuters.com/article/us-autos-takata/senators-to-press-au      5729 Characters\n",
      "ca.reuters.com/article/businessNews/idCAKCN1G10SW-OCABS           3285 Characters\n",
      "www.reuters.com/article/us-autos-takata/automakers-knew-earl      5077 Characters\n",
      "in.reuters.com/article/autos-takata/automakers-knew-earlier-      5116 Characters\n",
      "www.reuters.com/article/us-takata-bankruptcy-hearing/takata-      4530 Characters\n",
      "www.reuters.com/article/us-takata-bankruptcy-settlement/auto      6409 Characters\n",
      "in.reuters.com/article/takata-bankruptcy-settlement/automake      6382 Characters\n",
      "www.reuters.com/article/us-takata-bankruptcy-ruling/judge-ap      3699 Characters\n",
      "www.reuters.com/article/us-takata-sale-key-safety-systems/ke      5947 Characters\n",
      "ca.reuters.com/article/businessNews/idCAKCN1FX2VL-OCABS           6071 Characters\n",
      "www.reuters.com/article/us-takata-bankruptcy-settlement/taka      5005 Characters\n",
      "www.reuters.com/article/takata-pricefixing/south-africa-anti      2338 Characters\n",
      "www.reuters.com/article/takata-bankruptcy-settlement/takata-      4756 Characters\n",
      "www.usatoday.com/story/money/cars/2018/04/12/takata-acquired      7506 Characters\n",
      "www.usatoday.com/story/money/cars/2018/02/12/takata-settles-      6768 Characters\n",
      "www.usatoday.com/story/money/cars/2018/02/12/air-bag-danger-      7230 Characters\n",
      "www.usatoday.com/story/money/cars/2018/01/31/fords-china-ceo     11527 Characters\n",
      "money.cnn.com/2018/02/27/news/companies/takata-airbags-austr      6915 Characters\n",
      "money.cnn.com/2018/03/19/news/companies/hyundai-kia-airbag-i      6193 Characters\n",
      "www.wsj.com/articles/takata-whistleblower-claimants-settle-f     17649 Characters\n",
      "www.wsj.com/articles/takata-settles-with-drivers-injured-by-     17240 Characters\n",
      "www.wsj.com/articles/takata-settles-joint-probe-by-u-s-state     17632 Characters\n",
      "www.wsj.com/articles/regulator-car-executives-to-testify-at-     22954 Characters\n",
      "www.wsj.com/articles/more-auto-makers-sued-over-exploding-ta     17614 Characters\n",
      "www.wsj.com/articles/senators-press-car-executives-regulator     17627 Characters\n",
      "www.wsj.com/articles/u-s-investigates-failing-air-bags-in-hy     17556 Characters\n",
      "www.wsj.com/articles/companies-everywhere-copied-japanese-ma     17847 Characters\n",
      "abcnews.go.com/Business/wireStory/takata-acquired-key-safety     79945 Characters\n",
      "abcnews.go.com/Business/wireStory/hondas-profit-climbs-growi      9032 Characters\n",
      "www.nytimes.com/2018/02/22/business/takata-airbags-settlemen     26118 Characters\n",
      "www.nytimes.com/2018/02/11/business/takata-bankruptcy-airbag      5200 Characters\n",
      "www.nytimes.com/2018/02/28/briefing/xi-jinping-jared-kushner     11563 Characters\n",
      "www.nytimes.com/2018/02/23/business/dealbook/business-gun-co     45229 Characters\n",
      "www.washingtonpost.com/world/asia_pacific/takata-acquired-by      2194 Characters\n",
      "www.washingtonpost.com/world/asia_pacific/hondas-profit-clim      2856 Characters\n",
      "www.washingtonpost.com/news/the-switch/wp/2018/03/14/apple-g      5515 Characters\n",
      "www.washingtonpost.com/business/economy/amazon-trimming-jobs      5642 Characters\n",
      "www.washingtonpost.com/news/dr-gridlock/wp/2018/04/27/shed-s      5919 Characters\n",
      "www.nbcnews.com/news/us-news/hyundai-kia-under-scrutiny-air-      7243 Characters\n"
     ]
    }
   ],
   "source": [
    "for i in range(df_www.shape[0]):\n",
    "    short_url = df_www.iloc[i]['url']\n",
    "    short_url = short_url.replace(\"https://\", \"\")\n",
    "    short_url = short_url.replace(\"http://\", \"\")\n",
    "    short_url = short_url[0:60]\n",
    "    page_char = len(df_www.iloc[i]['text'])\n",
    "    print(\"{:<60s}{:>10d} Characters\".format(short_url, page_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Articles....    45\n",
      "Number of Terms.......  1107\n"
     ]
    }
   ],
   "source": [
    "n_reviews = len(df_www['text'])\n",
    "cv = CountVectorizer(max_df=0.7, min_df=4, max_features=None,\\\n",
    "analyzer=my_analyzer)\n",
    "tf = cv.fit_transform(df_www['text'])\n",
    "terms = cv.get_feature_names()\n",
    "print('{:.<22s}{:>6d}'.format(\"Number of Articles\", n_reviews))\n",
    "print('{:.<22s}{:>6d}'.format(\"Number of Terms\", len(terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Corpus contains a total of  1107  unique terms.\n",
      "The total number of terms in the Corpus is 19743\n",
      "\n",
      "Terms with Highest Frequency:\n",
      "tablet         171.000000\n",
      "video          168.000000\n",
      "reuters        164.000000\n",
      "wsj            145.000000\n",
      "browser        135.000000\n",
      "bankruptcy     129.000000\n",
      "new            129.000000\n",
      "inflator       119.000000\n",
      "deal           115.000000\n",
      "ford           114.000000\n"
     ]
    }
   ],
   "source": [
    "td = term_dic(tf, terms)\n",
    "print(\"The Corpus contains a total of \", len(td), \" unique terms.\")\n",
    "print(\"The total number of terms in the Corpus is\", sum(td.values()))\n",
    "term_sums = tf.sum(axis=0)\n",
    "term_counts = []\n",
    "for i in range(len(terms)):\n",
    "    term_counts.append([terms[i], term_sums[0,i]])\n",
    "def sortSecond(e):\n",
    "    return e[1]\n",
    "term_counts.sort(key=sortSecond, reverse=True)\n",
    "print(\"\\nTerms with Highest Frequency:\")\n",
    "for i in range(10):\n",
    "    print('{:<15s}{:>5f}'.format(term_counts[i][0], term_counts[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conducting Term/Frequency Matrix using TF-IDF\n",
      "The Term/Frequency matrix has 45  rows, and 1107  columns.\n",
      "The Term list has 1107  terms.\n",
      "\n",
      "Terms with Highest TF-IDF Scores:\n",
      "wsj              366.28\n",
      "tablet           313.43\n",
      "video            307.93\n",
      "reuters          284.97\n",
      "journal          247.55\n",
      "browser          234.58\n",
      "inflator         206.77\n",
      "new              197.73\n",
      "ford             193.02\n",
      "deal             189.82\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConducting Term/Frequency Matrix using TF-IDF\")\n",
    "# Default for norm is 'l2', use norm=None to supress\n",
    "tfidf_vect = TfidfTransformer(norm=None, use_idf=True) #set norm=None\n",
    "# tf matrix is (n_reviews)x(m_features\n",
    "tf = tfidf_vect.fit_transform(tf)\n",
    "term_idf_sums = tf.sum(axis=0)\n",
    "term_idf_scores = []\n",
    "for i in range(len(terms)):\n",
    "    term_idf_scores.append([terms[i], term_idf_sums[0,i]])\n",
    "print(\"The Term/Frequency matrix has\", tf.shape[0], \" rows, and\",\\\n",
    "    tf.shape[1], \" columns.\")\n",
    "print(\"The Term list has\", len(terms), \" terms.\")\n",
    "term_idf_scores.sort(key=sortSecond, reverse=True)\n",
    "print(\"\\nTerms with Highest TF-IDF Scores:\")\n",
    "for i in range(10):\n",
    "    j = i\n",
    "    print('{:<15s}{:>8.2f}'.format(term_idf_scores[j][0], \\\n",
    "    term_idf_scores[j][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** GENERATED TOPICS **********\n",
      "Topic #1: \n",
      "+wsj           +video         +journal       +podcast       +real          \n",
      "+art           +section       +estate        +jones         +dow           \n",
      "+street        +popular       +deal          +commercial    +subscribe     \n",
      "Topic #2: \n",
      "+time          +york          +new           +art           +opinion       \n",
      "+page          +today         +american      +navigation    +state         \n",
      "+subscription  +subscribe     +video         +event         +image         \n",
      "Topic #3: \n",
      "+ford          +mile          +inflator      +mazda         +pickup        \n",
      "+drive         +setting       +president     +tablet        +usa           \n",
      "+china         +sale          +linkedin      +reuters       +know          \n",
      "Topic #4: \n",
      "+honda         +yen           +sale          +cost          +grow          \n",
      "+cut           +percent       +nearly        +trillion      +january       \n",
      "+year          +annual        +march         +sell          +make          \n",
      "Topic #5: \n",
      "+apr           +trump         +white         +policy        +house         \n",
      "+name          +man           +feature       +online        +week          \n",
      "+turn          +president     +ad            +morning       +image         \n",
      "Topic #6: \n",
      "+tablet        +reuters       +browser       +landscape     +inflator      \n",
      "+portrait      +wide          +honda         +automaker     +bankruptcy    \n",
      "+thomson       +trust         +motor         +unit          +injure        \n",
      "Topic #7: \n",
      "+hyundai       +kia           +bank          +crash         +public        \n",
      "+brand         +nhtsa         +new           +make          +control       \n",
      "+index         +dow           +percent       +car           +story         \n"
     ]
    }
   ],
   "source": [
    "n_topics = 7\n",
    "\n",
    "uv = LatentDirichletAllocation(n_components=n_topics, \\\n",
    "learning_method='online', random_state=12345)\n",
    "U = uv.fit_transform(tf)\n",
    "\n",
    "print(\"\\n********** GENERATED TOPICS **********\")\n",
    "\n",
    "display_topics(uv.components_, terms, n_terms=15)\n",
    "# Store topic selection for each doc in topics[]\n",
    "topics = [0] * n_reviews\n",
    "for i in range(n_reviews):\n",
    "    max = abs(U[i][0])\n",
    "    topics[i] = 0\n",
    "    for j in range(n_topics):\n",
    "        x = abs(U[i][j])\n",
    "        if x > max:\n",
    "            max = x\n",
    "            topics[i] = j\n",
    "U_rev_scores = []\n",
    "for i in range(n_reviews):\n",
    "    u = [0] * (n_topics+1)\n",
    "    u[0] = topics[i]\n",
    "    for j in range(n_topics):\n",
    "        u[j+1] = U[i][j]\n",
    "    U_rev_scores.append(u)\n",
    "rev_scores = U_rev_scores\n",
    "# Integrate Topic Scores into Main Data Frame (df)\n",
    "cols = [\"topic\"]\n",
    "for i in range(n_topics):\n",
    "    s = \"T\"+str(i+1)\n",
    "    cols.append(s)\n",
    "df_rev = pd.DataFrame.from_records(rev_scores, columns=cols)\n",
    "df_www = df_www.join(df_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reuters.com/article/us-autos-takat...</td>\n",
       "      <td>Honda, Ford to testify at U.S. Senate Takata h...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.998709</td>\n",
       "      <td>0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://in.reuters.com/article/autos-takata/ho...</td>\n",
       "      <td>Honda, Ford to testify at U.S. Senate Takata h...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.998717</td>\n",
       "      <td>0.000214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.reuters.com/article/us-autos-takat...</td>\n",
       "      <td>U.S. senators call new hearing on Takata auto ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.998568</td>\n",
       "      <td>0.000239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.reuters.com/article/us-takata-whis...</td>\n",
       "      <td>Takata whistleblowers to share $1.7 million aw...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.697399</td>\n",
       "      <td>0.301479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://uk.reuters.com/article/uk-autos-takata...</td>\n",
       "      <td>Honda, Ford to testify at U.S. Senate Takata h...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.998735</td>\n",
       "      <td>0.000211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                url  \\\n",
       "0           0  https://www.reuters.com/article/us-autos-takat...   \n",
       "1           1  https://in.reuters.com/article/autos-takata/ho...   \n",
       "2           2  https://www.reuters.com/article/us-autos-takat...   \n",
       "3           3  https://www.reuters.com/article/us-takata-whis...   \n",
       "4           4  https://uk.reuters.com/article/uk-autos-takata...   \n",
       "\n",
       "                                                text  topic        T1  \\\n",
       "0  Honda, Ford to testify at U.S. Senate Takata h...      5  0.000215   \n",
       "1  Honda, Ford to testify at U.S. Senate Takata h...      5  0.000214   \n",
       "2  U.S. senators call new hearing on Takata auto ...      5  0.000238   \n",
       "3  Takata whistleblowers to share $1.7 million aw...      5  0.000224   \n",
       "4  Honda, Ford to testify at U.S. Senate Takata h...      5  0.000211   \n",
       "\n",
       "         T2        T3        T4        T5        T6        T7  \n",
       "0  0.000215  0.000215  0.000215  0.000215  0.998709  0.000215  \n",
       "1  0.000214  0.000214  0.000214  0.000214  0.998717  0.000214  \n",
       "2  0.000239  0.000239  0.000238  0.000239  0.998568  0.000239  \n",
       "3  0.000224  0.000225  0.000224  0.000225  0.697399  0.301479  \n",
       "4  0.000211  0.000211  0.000211  0.000211  0.998735  0.000211  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_www.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews.....    45\n",
      "Number of Terms....... 21045\n",
      "Overall Average Sentiment:  0.07861443970773985\n"
     ]
    }
   ],
   "source": [
    "sent_score(tf,df_www, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reuters.com/article/us-autos-takat...</td>\n",
       "      <td>Honda, Ford to testify at U.S. Senate Takata h...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.998709</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://in.reuters.com/article/autos-takata/ho...</td>\n",
       "      <td>Honda, Ford to testify at U.S. Senate Takata h...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.998717</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.161290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.reuters.com/article/us-autos-takat...</td>\n",
       "      <td>U.S. senators call new hearing on Takata auto ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.998568</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.reuters.com/article/us-takata-whis...</td>\n",
       "      <td>Takata whistleblowers to share $1.7 million aw...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.697399</td>\n",
       "      <td>0.301479</td>\n",
       "      <td>0.141304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://uk.reuters.com/article/uk-autos-takata...</td>\n",
       "      <td>Honda, Ford to testify at U.S. Senate Takata h...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.998735</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                url  \\\n",
       "0           0  https://www.reuters.com/article/us-autos-takat...   \n",
       "1           1  https://in.reuters.com/article/autos-takata/ho...   \n",
       "2           2  https://www.reuters.com/article/us-autos-takat...   \n",
       "3           3  https://www.reuters.com/article/us-takata-whis...   \n",
       "4           4  https://uk.reuters.com/article/uk-autos-takata...   \n",
       "\n",
       "                                                text  topic        T1  \\\n",
       "0  Honda, Ford to testify at U.S. Senate Takata h...      5  0.000215   \n",
       "1  Honda, Ford to testify at U.S. Senate Takata h...      5  0.000214   \n",
       "2  U.S. senators call new hearing on Takata auto ...      5  0.000238   \n",
       "3  Takata whistleblowers to share $1.7 million aw...      5  0.000224   \n",
       "4  Honda, Ford to testify at U.S. Senate Takata h...      5  0.000211   \n",
       "\n",
       "         T2        T3        T4        T5        T6        T7  Sentiment_Score  \n",
       "0  0.000215  0.000215  0.000215  0.000215  0.998709  0.000215         0.000000  \n",
       "1  0.000214  0.000214  0.000214  0.000214  0.998717  0.000214         0.161290  \n",
       "2  0.000239  0.000239  0.000238  0.000239  0.998568  0.000239        -0.064516  \n",
       "3  0.000224  0.000225  0.000224  0.000225  0.697399  0.301479         0.141304  \n",
       "4  0.000211  0.000211  0.000211  0.000211  0.998735  0.000211         0.100000  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_www.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
